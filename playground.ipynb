{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self):\n",
    "        self.token_to_id = {\"[PAD]\": 0, \"[UNK]\": 1}\n",
    "        self.num_tokens = 2\n",
    "    \n",
    "    # Fit the tokenizer onto the dataset\n",
    "    def fit(self, texts):\n",
    "        for text in texts:\n",
    "            for word in re.findall(r'\\w+', text):\n",
    "                if word not in self.token_to_id:\n",
    "                    self.token_to_id[word] = self.num_tokens\n",
    "                    self.num_tokens += 1\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return_list = []\n",
    "        for word in re.findall(r'\\w+', text):\n",
    "            return_list.append(self.token_to_id.get(word, 1))\n",
    "        return return_list\n",
    "    \n",
    "    def untokenize(self, token_list):\n",
    "        return_string = \"\"\n",
    "        for token in token_list:\n",
    "            return_string += self.id_to_token.get(token, \"[UNK]\") + \" \"\n",
    "        return return_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fawaz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, filename, is_multi_line=True, chunk_size=1024):\n",
    "        self.chunks = []\n",
    "        with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "            if is_multi_line:\n",
    "                self.chunks = f.readlines()\n",
    "            else:\n",
    "                while True:\n",
    "                    chunk = f.read(chunk_size)\n",
    "                    if not chunk:\n",
    "                        break  # eof\n",
    "                    self.chunks.append(chunk)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.chunks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3]\n"
     ]
    }
   ],
   "source": [
    "# Example Usage:\n",
    "texts = [\"Hello world!\", \"How are you?\"]\n",
    "tokenizer = SimpleTokenizer()\n",
    "tokenizer.fit(texts)\n",
    "tokenized_text = tokenizer.tokenize(\"Hello world!\")\n",
    "print(tokenized_text)  # Output: [2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage:\n",
    "dataset = TextDataset('datasets/iac_mini.txt', is_multi_line=False)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "texts = dataset.chunks\n",
    "tokenizer.fit(texts)\n",
    "\n",
    "# Process data in batches\n",
    "for batch in dataloader:\n",
    "    for line in batch:\n",
    "        tokens = tokenizer.tokenize(line)\n",
    "        # print(f\"Original text: {line}\")\n",
    "        # print(f\"Tokenized text: {tokens}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Embedding dim is how many \"features\" you're allowing the model to use\n",
    "# to describe the vocabulary\n",
    "vocab_size = tokenizer.num_tokens\n",
    "embedding_dim = 128\n",
    "\n",
    "embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "max_context_len = 512\n",
    "# Create positional encodings to let the model learn the value of positions\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Create positional encodings\n",
    "        # Define a tensor of context length by input embedding dim\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # Create an array of position values for the context len [[0], [1], [2],..., [max_len]]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # Purely a scaling factor for our tensors so that the values don't get blown up to a bajillion\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encodings to the input embeddings\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import torch\n",
    "embed_dim = 4\n",
    "max_len = 8\n",
    "pe = torch.zeros(max_len, embed_dim)\n",
    "# print(pe)\n",
    "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "# print(position)\n",
    "z = torch.arange(0, embed_dim, 2).float()\n",
    "y = torch.tensor(10000.0)\n",
    "x = (-torch.log(torch.tensor(10000.0)) / embed_dim)\n",
    "div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / embed_dim))\n",
    "myx = position*div_term\n",
    "# print(f\"pos * div = {myx}\")\n",
    "# print(f\"sin = {torch.sin(myx)}\")\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "pe[:, 1::2] = torch.cos(position * div_term)\n",
    "print(f\"Positional Encoding: {pe}\")\n",
    "pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "print(f\"Positional Encoding: {pe}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        \n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "        \n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Run throught the initial linear layers with the input\n",
    "        queries = self.queries(x)\n",
    "        keys = self.keys(x)\n",
    "        values = self.values(x)\n",
    "\n",
    "        # Splitting into multiple heads\n",
    "        queries = queries.view(x.size(0), -1, self.heads, self.head_dim)\n",
    "        keys = keys.view(x.size(0), -1, self.heads, self.head_dim)\n",
    "        values = values.view(x.size(0), -1, self.heads, self.head_dim)\n",
    "\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        out = torch.matmul(attention_weights, values)\n",
    "        out = self.fc_out(torch.cat(out, dim=2))\n",
    "        \n",
    "        return out\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# Define the input tensor x\n",
    "x = torch.tensor([[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\n",
    "                  [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]], dtype=torch.float32)\n",
    "\n",
    "# Define the linear transformation layers for queries, keys, and values\n",
    "class LinearLayers(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.queries = torch.nn.Linear(4, 4)\n",
    "        self.keys = torch.nn.Linear(4, 4)\n",
    "        self.values = torch.nn.Linear(4, 4)\n",
    "        self.fc_out = torch.nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.queries(x)\n",
    "        keys = self.keys(x)\n",
    "        values = self.values(x)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        queries = queries.view(x.size(0), -1, 2, 2)  # Splitting the last dimension into (heads, head_dim)\n",
    "        keys = keys.view(x.size(0), -1, 2, 2)\n",
    "        values = values.view(x.size(0), -1, 2, 2)\n",
    "\n",
    "        # Compute attention scores\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / math.sqrt(2)\n",
    "\n",
    "        # Softmax normalization\n",
    "        attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Compute output\n",
    "        out = torch.matmul(attention_weights, values)\n",
    "\n",
    "        # Concatenating heads\n",
    "        out = out.view(x.size(0), -1, 4)\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "# Instantiate the model and run the forward pass\n",
    "model = LinearLayers()\n",
    "output = model(x)\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
