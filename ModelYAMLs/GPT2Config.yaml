MODEL: "GPT2"

MODEL_SETUP:
  ARCHITECTURE: "Char2ByteDecoder"

MODEL_PARAMS:
  NUM_HEADS: 12
  NUM_LAYERS: 12
  BATCH_SIZE: 32
  CONTEXT_LENGTH: 32
  EMBEDDING_SIZE: 768
  DROPOUT: 0.2

DATA_PARAMS:
  TRAIN_SPLIT: 0.8
  VAL_SPLIT: 0.1  

TRAINING_PARAMS:
  LEARNING_RATE: 0.0003
  LR_DECAY: 0.99
  NUM_EPOCHS: 3
  ESTIMATION_AMT: 1000

DATASET: "Datasets/shakespeare.txt"
DATASET_LOADER: "SimpleTextDataset"

TOKENIZER: "SingleLetterTokenizer"

LOSS: "CrossEntropyLoss"
OPTIMIZER: "Adam"